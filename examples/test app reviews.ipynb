{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up package from parent folder\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(\n",
    "    inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "import gensim.downloader as model_api\n",
    "\n",
    "import ordinal\n",
    "from ordinal import OrderedProbitRanker\n",
    "from ordinal import logit\n",
    "\n",
    "import AppReviews # generate review data from app store\n",
    "import docembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.284942988976365e-06, array([ 4.11417497e-06, -3.22979281e-06, -7.48958864e-06, -2.69568096e-06]))\n",
      "3.284942988976365e-06\n",
      "[-18.27416671  15.51798264   3.08541305 -26.53379904]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_blobs(n_samples=300, random_state=42)\n",
    "X, y = shuffle(X, y, random_state=7)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "opr = OrderedProbitRanker(method='L-BFGS-B', use_grad=False)\n",
    "opr.fit(X, y)\n",
    "\n",
    "ymasks = np.array([np.array(y == c_) for c_ in opr.classes_])\n",
    "betas = np.concatenate([opr.cuts_, opr.coef_])\n",
    "\n",
    "print(opr._ordered_probit_loss_and_grad(betas, ymasks, X))\n",
    "print(opr._orderedProbitLogLike(betas, ymasks, X))\n",
    "print(betas)\n",
    "print(opr.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.284942988976365e-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = ymasks.shape[1]\n",
    "n_cuts = ymasks.shape[0] - 1\n",
    "# TODO: xb can \"explode\" out of normal cdf bounds\n",
    "#       eg. values above 8 and below -8 have cdf of 0 and 1 \n",
    "#       regardless of cutoff point\n",
    "xb = X @ betas[n_cuts:]\n",
    "# ensure cutpoints remain ordered\n",
    "# TODO: This can be done by reparametrizing the cutpoints...\n",
    "cuts = np.sort(betas[:n_cuts])\n",
    "# cdf up to cutpoints\n",
    "cdf_areas = [norm.cdf(ct - xb) for ct in cuts]\n",
    "# last cdf area is from last cutpoint on\n",
    "cdf_areas.append(cdf_areas[-1])\n",
    "cdf_areas = np.array(cdf_areas)\n",
    "# pdf areas between cutpoints = cdf[i] - cdf[i-1]\n",
    "pdf_areas = np.empty_like(ymasks, dtype='float')\n",
    "# first is cdf[cut_0] - 0\n",
    "pdf_areas[0] = cdf_areas[0]\n",
    "# last is 1 - cdf[last_cut]\n",
    "pdf_areas[-1] = 1 - cdf_areas[-1]\n",
    "# middle cuts are cdf area between each\n",
    "for i in range(1, n_cuts):\n",
    "    pdf_areas[i] = cdf_areas[i] - cdf_areas[i-1]\n",
    "res = np.zeros(n_samples)\n",
    "for i in range(len(ymasks)):\n",
    "    res += (ymasks[i] * pdf_areas[i])\n",
    "res = np.sum(np.log(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OrderedProbit-master\\ordinalranker\\ranking_model.py:226: OptimizeWarning: Unknown solver options: maxfun\n",
      "  options={\"disp\":True, 'maxiter':50000, \"maxfun\":150000})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 484.697248\n",
      "         Iterations: 7204\n",
      "         Function evaluations: 9189\n",
      "(484.69724782927943, array([ 2.72691938e-01, -2.16137996e-01, -2.65049319e-01, -9.26084054e-02,\n",
      "       -6.56931209e+01, -3.73561284e+03, -5.47352571e+03, -7.48491604e+03,\n",
      "        1.11983610e+01, -3.64042603e+02, -4.51658021e+03, -7.20604206e+04,\n",
      "       -3.44954919e+03, -7.62543900e+03, -2.88667046e+05, -1.11560617e+04,\n",
      "       -2.13973401e+05, -1.47182366e+04]))\n",
      "484.69724782927943\n",
      "[-3.62323115e+00 -1.80464505e+00 -2.60133094e-02  8.97345799e-01\n",
      "  1.78909737e+00 -4.07535387e-02  1.51953928e-02 -3.96980741e-02\n",
      "  2.38329471e-01 -9.00211940e-02  6.05754052e-01 -9.84447896e-03\n",
      " -4.32566398e-01  5.54392463e-02 -3.62701496e-03 -2.97064989e-02\n",
      "  2.44484279e-03 -1.58934117e-01]\n",
      "0.6086956521739131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes, load_linnerud\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "N_CLASSES = 6\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "kbd = KBinsDiscretizer(n_bins=N_CLASSES, encode='ordinal', strategy='kmeans')\n",
    "y = kbd.fit_transform(y.reshape(-1, 1)).flatten().astype(np.int32)\n",
    "\n",
    "opr = OrderedProbitRanker(method='nelder-mead', use_grad=False)\n",
    "opr.fit(X, y)\n",
    "\n",
    "ymasks = np.array([np.array(y == c_) for c_ in opr.classes_])\n",
    "betas = np.concatenate([opr.cuts_, opr.coef_])\n",
    "\n",
    "print(opr._ordered_probit_loss_and_grad(betas, ymasks, X))\n",
    "print(opr._orderedProbitLogLike(betas, ymasks, X))\n",
    "print(betas)\n",
    "print(opr.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-484.69724782927943"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = ymasks.shape[1]\n",
    "n_cuts = ymasks.shape[0] - 1\n",
    "# TODO: xb can \"explode\" out of normal cdf bounds\n",
    "#       eg. values above 8 and below -8 have cdf of 0 and 1 \n",
    "#       regardless of cutoff point\n",
    "xb = X @ betas[n_cuts:]\n",
    "# ensure cutpoints remain ordered\n",
    "# TODO: This can be done by reparametrizing the cutpoints...\n",
    "cuts = np.sort(betas[:n_cuts])\n",
    "# cdf up to cutpoints\n",
    "cdf_areas = [norm.cdf(ct - xb) for ct in cuts]\n",
    "# last cdf area is from last cutpoint on\n",
    "cdf_areas.append(cdf_areas[-1])\n",
    "cdf_areas = np.array(cdf_areas)\n",
    "# pdf areas between cutpoints = cdf[i] - cdf[i-1]\n",
    "pdf_areas = np.empty_like(ymasks, dtype='float')\n",
    "# first is cdf[cut_0] - 0\n",
    "pdf_areas[0] = cdf_areas[0]\n",
    "# last is 1 - cdf[last_cut]\n",
    "pdf_areas[-1] = 1 - cdf_areas[-1]\n",
    "# middle cuts are cdf area between each\n",
    "for i in range(1, n_cuts):\n",
    "    pdf_areas[i] = cdf_areas[i] - cdf_areas[i-1]\n",
    "res = np.zeros(n_samples)\n",
    "for i in range(len(ymasks)):\n",
    "    res += (ymasks[i] * pdf_areas[i])\n",
    "res = np.sum(np.log(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4954757c312d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv(\"cache.csv\")\n",
    "X = dft.drop('rating', 1)\n",
    "y = dft.rating\n",
    "\n",
    "opr = OrderedProbitRanker(method='L-BFGS-B', use_grad=False)\n",
    "opr.fit(X, y)\n",
    "\n",
    "ymasks = np.array([np.array(y == c_) for c_ in opr.classes_])\n",
    "betas = np.concatenate([opr.cuts_, opr.coef_])\n",
    "\n",
    "print(opr._ordered_probit_loss_and_grad(betas, ymasks, X))\n",
    "print(opr._orderedProbitLogLike(betas, ymasks, X))\n",
    "print(opr.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"app_reviews.csv\")\n",
    "df.rating = df.rating.astype(int)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model_api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "dft = df.join(pd.get_dummies(df.app_name, drop_first=True, dummy_na=True))\n",
    "\n",
    "# Add embedding components\n",
    "for col in ['title', 'review']:\n",
    "    tokens = docembedding.stringprocessing.tokenize(dft['review'], lower=True, split=True)\n",
    "    weights = docembedding.embedding.getWordWeights(tokens, \"tf-idf\")\n",
    "    embeds = docembedding.embedding.sentenceEmbedding(tokens, model, weights)\n",
    "    embeds = pd.DataFrame(embeds)\n",
    "    embeds.columns = [col + str(colnum) for colnum in embeds.columns]\n",
    "    dft = dft.join(embeds)\n",
    "dft = dft.drop(['version', 'vote_count', 'review', 'title', 'app_name'], 1)\n",
    "dft.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dft.drop('rating', 1)\n",
    "y = dft.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitscore(model, X, y):\n",
    "    start = time.time()\n",
    "    model.fit(X, y)\n",
    "    pred_val = model.predict(X)\n",
    "    print(\"time: \", time.time() - start)\n",
    "    print(\"score: \", metrics.accuracy_score(pred_val, y))\n",
    "    print(\"mse: \", metrics.mean_squared_error(pred_val, y))\n",
    "    print(\"mae: \", metrics.mean_absolute_error(pred_val, y))\n",
    "    pd.Series(pred_val).hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LogisticRegression(C=9999)\n",
    "fitscore(lr, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = mord.LogisticAT(alpha=1, verbose=1, max_iter=10000000)\n",
    "fitscore(lat, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit = mord.LogisticIT(alpha=1, verbose=1, max_iter=10000000)\n",
    "fitscore(lit, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lse = mord.LogisticSE(alpha=1, verbose=1, max_iter=10000000)\n",
    "fitscore(lse, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = OrderedProbitRanker(method='L-BFGS-B', use_grad=False)\n",
    "fitscore(opr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opr = OrderedProbitRanker(method='L-BFGS-B', use_grad=False)\n",
    "start = time.time()\n",
    "opr.fit(X, y)\n",
    "print(\"time: \", time.time() - start)\n",
    "print(\"score: \", opr.score(X, y))\n",
    "print(\"cuts: \", opr.cuts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cuts = len(opr.cuts_)\n",
    "n_samples = len(X)\n",
    "betas = opr.coef_\n",
    "\n",
    "xb = X @ opr.coef_\n",
    "ymasks = np.array([np.array(y == c_) for c_ in opr.classes_])\n",
    "cdf_areas = [norm.cdf(ct - xb) for ct in opr.cuts_]\n",
    "cdf_areas.append(cdf_areas[-1])\n",
    "cdf_areas = np.array(cdf_areas)\n",
    "pdf_areas = np.zeros_like(ymasks, dtype='float')\n",
    "pdf_areas[0] = cdf_areas[0]\n",
    "pdf_areas[-1] = 1 - cdf_areas[-1]\n",
    "for i in range(1, n_cuts):\n",
    "    pdf_areas[i] = cdf_areas[i] - cdf_areas[i-1]\n",
    "res = np.zeros(n_samples)\n",
    "for i in range(len(ymasks)):\n",
    "    res += (ymasks[i] * pdf_areas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.Series(res)\n",
    "ymk = pd.DataFrame(ymasks.T)\n",
    "pdf = pd.DataFrame(pdf_areas.T)\n",
    "cdf = pd.DataFrame(cdf_areas.T)\n",
    "bad = (out == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
